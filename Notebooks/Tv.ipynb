{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Text Generation with GPT-2:\n",
    "\n",
    "Use the Hugging Face Transformers library to generate text content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am going to create an ai-powered youtube channel to share my experiences with the community. I will be posting videos and videos of my experiences with the community. I will also be posting videos of my experiences with the community.\n",
      "\n",
      "I\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model_name = 'gpt2'\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "def generate_text(prompt, max_length=50):\n",
    "    # Encode the prompt\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    \n",
    "    # Create attention mask\n",
    "    attention_mask = torch.ones(input_ids.shape, dtype=torch.long)\n",
    "    \n",
    "    # Generate text\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_length=max_length,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    # Decode the generated text\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "# Example usage\n",
    "prompt = \"I am going to create an ai-powered youtube channel\"\n",
    "\n",
    "generated_text = generate_text(prompt)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I ant to create an AI-Powered Generative YoutubeChannel Â ( http://www.youtube.com/channel/UCXJXJXJXJXJXJXJXJXJXJXJX\n"
     ]
    }
   ],
   "source": [
    "prompt = \"I ant to create an AI-Powered Generative YoutubeChannel \"\n",
    "generated_text = generate_text(prompt)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download and Process Open-Source Content:\n",
    "\n",
    "Use Python scripts to download and process open-source videos and images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def download_file(url, filename):\n",
    "    response = requests.get(url)\n",
    "    with open(filename, 'wb') as file:\n",
    "        file.write(response.content)\n",
    "\n",
    "# Example usage\n",
    "download_file(\"https://example.com/video.mp4\", \"downloaded_video.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Video Creation with MoviePy:\n",
    "\n",
    "Create and edit videos using MoviePy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.editor import VideoFileClip, concatenate_videoclips, TextClip, CompositeVideoClip\n",
    "\n",
    "# Load video clips\n",
    "clip1 = VideoFileClip(\"video1.mp4\")\n",
    "clip2 = VideoFileClip(\"video2.mp4\")\n",
    "\n",
    "# Concatenate video clips\n",
    "final_clip = concatenate_videoclips([clip1, clip2])\n",
    "\n",
    "# Add text overlay\n",
    "txt_clip = TextClip(\"AI-Powered TV Channel\", fontsize=70, color='white')\n",
    "txt_clip = txt_clip.set_pos('center').set_duration(10)\n",
    "video = CompositeVideoClip([final_clip, txt_clip])\n",
    "\n",
    "# Save the final video\n",
    "video.write_videofile(\"final_video.mp4\", fps=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Combine AI-Generated and Open-Source Content:\n",
    "Use MoviePy and OpenCV to combine AI-generated content with open-source content.\"\"\"\n",
    "\n",
    "\n",
    "import cv2\n",
    "\n",
    "# Load AI-generated text as an image\n",
    "text = \"AI-Generated Content\"\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "img = cv2.putText(np.zeros((500, 500, 3), dtype=np.uint8), text, (50, 250), font, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "# Save the image\n",
    "cv2.imwrite(\"ai_generated_image.jpg\", img)\n",
    "\n",
    "# Combine with open-source video\n",
    "clip = VideoFileClip(\"downloaded_video.mp4\")\n",
    "img_clip = ImageClip(\"ai_generated_image.jpg\").set_duration(clip.duration)\n",
    "final_clip = CompositeVideoClip([clip, img_clip.set_pos(\"center\")])\n",
    "\n",
    "# Save the final video\n",
    "final_clip.write_videofile(\"combined_video.mp4\", fps=24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Document the Process\n",
    "Write Blog Posts:\n",
    "\n",
    "Document each step of the project and share your insights on LinkedIn and Medium.\n",
    "Include code snippets, screenshots, and explanations.\n",
    "Create Tutorials:\n",
    "\n",
    "Create video tutorials to demonstrate how you built the AI-powered TV channel.\n",
    "Share these tutorials on YouTube and other social media platforms.\n",
    "Step 5: Promote Your Content\n",
    "Social Media Promotion:\n",
    "\n",
    "Share your blog posts, tutorials, and videos on social media platforms.\n",
    "Engage with your audience by responding to comments and messages.\n",
    "SEO and Analytics:\n",
    "\n",
    "Optimize your content for search engines to increase visibility.\n",
    "Use analytics tools to track engagement and identify areas for improvement.\n",
    "Summary\n",
    "By following this roadmap, you can create an AI-powered generative online TV channel locally on your PC. This project will not only help you build your skills in AI and content creation but also enhance your online visibility and attract inbound calls. Good luck with your project!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Second part : \n",
    "I think what I want to start with is learning how to make deep fake. Tha way I could take several open source videos from youtube and mix them with genAI to make a deep fake. I guess I might even take any video from you tube if I am going to use it just for as context for my genAI. The same way I could use isntragram and tiktock videos. Myy idea is to exploit the fact that I watch a lot of videos on Youtube. I could then download the best content that i watch and create a list of topics from them. Then I could do is select a topic, start and 1)make the genAI generate a deepfake that talks about these topics with artifical presenters or as an easier first step 2) just make ai select segments from different videos and put them all together as a remix 3) option would eb create a deep fake that repeats content of a video in a new language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"Option 1: Generate a Deepfake with Artificial Presenters\n",
    "Step 1: Download and Prepare Videos\n",
    "Download Videos: Use a tool like youtube-dl to download videos from YouTube.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Download Videos: Use a tool like youtube-dl to download videos from YouTube.\n",
    "youtube-dl -f best -o \"videos/%(title)s.%(ext)s\" <video_url>\n",
    " \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Extract Frames: Use OpenCV to extract frames from the videos.\"\"\"\n",
    "import cv2\n",
    "\n",
    "def extract_frames(video_path, output_folder):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    count = 0\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        cv2.imwrite(f\"{output_folder}/frame{count:04d}.jpg\", frame)\n",
    "        count += 1\n",
    "    cap.release()\n",
    "\n",
    "extract_frames(\"videos/sample_video.mp4\", \"frames\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Step 2: Generate Deepfake\n",
    "Face Detection and Alignment: Use dlib and face_recognition to detect and align faces.\n",
    "\"\"\"\n",
    "\n",
    "import face_recognition\n",
    "\n",
    "def detect_faces(image_path):\n",
    "    image = face_recognition.load_image_file(image_path)\n",
    "    face_locations = face_recognition.face_locations(image)\n",
    "    return face_locations\n",
    "\n",
    "face_locations = detect_faces(\"frames/frame0000.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Step 2: Generate Deepfake\n",
    "\n",
    "1. **Face Detection and Alignment**: Use `dlib` and `face_recognition` to detect and align faces.\n",
    "   ```python\n",
    "   import face_recognition\n",
    "\n",
    "   def detect_faces(image_path):\n",
    "       image = face_recognition.load_image_file(image_path)\n",
    "       face_locations = face_recognition.face_locations(image)\n",
    "       return face_locations\n",
    "\n",
    "   face_locations = detect_faces(\"frames/frame0000.jpg\")\n",
    "   ```\n",
    "\n",
    "2. **Deepfake Generation**: Use a pre-trained model like `faceswap` or `DeepFaceLab` to generate deepfakes. These tools have their own setup and usage instructions.\n",
    "\n",
    "#### Step 3: Combine Frames into Video\n",
    "\n",
    "1. **Combine Frames**: Use OpenCV to combine frames back into a video.\n",
    "   ```python\n",
    "   def create_video(frame_folder, output_video):\n",
    "       images = [img for img in os.listdir(frame_folder) if img.endswith(\".jpg\")]\n",
    "       frame = cv2.imread(os.path.join(frame_folder, images[0]))\n",
    "       height, width, layers = frame.shape\n",
    "\n",
    "       video = cv2.VideoWriter(output_video, cv2.VideoWriter_fourcc(*'DIVX'), 30, (width, height))\n",
    "\n",
    "       for image in images:\n",
    "           video.write(cv2.imread(os.path.join(frame_folder, image)))\n",
    "\n",
    "       cv2.destroyAllWindows()\n",
    "       video.release()\n",
    "\n",
    "   create_video(\"frames\", \"output_video.avi\")\n",
    "   ```\n",
    "\n",
    "### Option 2: Remix Video Segments\n",
    "\n",
    "1. **Download Videos**: Use `youtube-dl` to download videos from YouTube.\n",
    "   ```bash\n",
    "   youtube-dl -f best -o \"videos/%(title)s.%(ext)s\" <video_url>\n",
    "   ```\n",
    "\n",
    "2. **Extract Segments**: Use MoviePy to extract segments from different videos.\n",
    "   ```python\n",
    "   from moviepy.editor import VideoFileClip, concatenate_videoclips\n",
    "\n",
    "   def extract_segment(video_path, start_time, end_time):\n",
    "       clip = VideoFileClip(video_path).subclip(start_time, end_time)\n",
    "       return clip\n",
    "\n",
    "   clip1 = extract_segment(\"videos/video1.mp4\", 0, 10)\n",
    "   clip2 = extract_segment(\"videos/video2.mp4\", 20, 30)\n",
    "   final_clip = concatenate_videoclips([clip1, clip2])\n",
    "   final_clip.write_videofile(\"remix_video.mp4\")\n",
    "   ```\n",
    "\n",
    "### Option 3: Create a Deepfake in a New Language\n",
    "\n",
    "1. **Download Videos**: Use `youtube-dl` to download videos from YouTube.\n",
    "   ```bash\n",
    "   youtube-dl -f best -o \"videos/%(title)s.%(ext)s\" <video_url>\n",
    "   ```\n",
    "\n",
    "2. **Transcribe and Translate**: Use a speech-to-text model to transcribe the video and then translate the text.\n",
    "   ```python\n",
    "   import speech_recognition as sr\n",
    "   from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "   def transcribe_audio(audio_path):\n",
    "       recognizer = sr.Recognizer()\n",
    "       audio_file = sr.AudioFile(audio_path)\n",
    "       with audio_file as source:\n",
    "           audio = recognizer.record(source)\n",
    "       return recognizer.recognize_google(audio)\n",
    "\n",
    "   def translate_text(text, src_lang, tgt_lang):\n",
    "       model_name = f'Helsinki-NLP/opus-mt-{src_lang}-{tgt_lang}'\n",
    "       model = MarianMTModel.from_pretrained(model_name)\n",
    "       tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "       translated = model.generate(**tokenizer(text, return_tensors=\"pt\", padding=True))\n",
    "       return tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "\n",
    "   transcription = transcribe_audio(\"videos/sample_audio.wav\")\n",
    "   translation = translate_text(transcription, \"en\", \"es\")\n",
    "   print(translation)\n",
    "   ```\n",
    "\n",
    "3. **Generate Deepfake with Translated Audio**: Use TTS to generate audio in the new language and create a deepfake with the translated audio.\n",
    "\n",
    "### Ethical Considerations\n",
    "\n",
    "- **Consent**: Ensure you have permission to use and modify the videos.\n",
    "- **Transparency**: Clearly indicate that the content is AI-generated.\n",
    "- **Avoid Harm**: Do not use deepfake technology to deceive or harm others.\n",
    "\n",
    "### Summary\n",
    "\n",
    "By following these steps, you can create deepfakes, remix video segments, and generate content in new languages using generative AI technologies. This project will help you build your skills in AI and content creation while leveraging the vast amount of video content available online."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
